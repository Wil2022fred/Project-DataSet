{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6bdb82a",
   "metadata": {},
   "source": [
    "**Data Science Challenge: Card Transactions!**\n",
    "This coding and analysis challenge is designed to test your skill and intuition analyzing real[-ish] world data. For the challenge, we will use credit card transactions data. Note that this dataset loosely resembles real transactional data from Capital One credit card customers, but the entities and relations within are purely fictional. No persons, places, or things lost their identity in the making of this dataset.\n",
    "\n",
    "**Required Questions:** Please answer completely all four required questions.\n",
    "\n",
    "\n",
    "**Question 1:** Load\\\n",
    "- Programmatically download and load into your favorite analytical tool the transactions data. This data, which is in line-delimited JSON format, can be found here\n",
    "\n",
    "- Please describe the structure of the data. Number of records and fields in each record?\n",
    "\n",
    "- Please provide some additional basic summary statistics for each field. Be sure to include a count of null, minimum, maximum, and unique values where appropriate.\n",
    "\n",
    "**Question 2:** Plot\\\n",
    "- Plot a histogram of the processed amounts of each transaction, the transactionAmount column.\n",
    "\n",
    "- Report any structure you find and any hypotheses you have about that structure.\n",
    "\n",
    "**Question 3:** Data Wrangling - Duplicate Transactions\\\n",
    "You will notice a number of what look like duplicated transactions in the data set. One type of duplicated transaction is a reversed transaction, where a purchase is followed by a reversal. Another example is a multi-swipe, where a vendor accidentally charges a customer's card multiple times within a short time span.\n",
    "\n",
    "- Can you programmatically identify reversed and multi-swipe transactions?\n",
    "\n",
    "- What total number of transactions and total dollar amount do you estimate for the reversed transactions? For the multi-swipe transactions? (please consider the first transaction to be \"normal\" and exclude it from the number of transaction and dollar amount counts)\n",
    "\n",
    "- Did you find anything interesting about either kind of transaction?\n",
    "\n",
    "**Question 4:** Model\\\n",
    "Fraud is a problem for any bank. Fraud can take many forms, whether it is someone stealing a single credit card, to large batches of stolen credit card numbers being used on the web, or even a mass compromise of credit card numbers stolen from a merchant via tools like credit card skimming devices.\n",
    "\n",
    "- Each of the transactions in the dataset has a field called isFraud. Please build a predictive model to determine whether a given transaction will be fraudulent or not. Use as much of the data as you like (or all of it).\n",
    "\n",
    "- Provide an estimate of performance using an appropriate sample, and show your work.\n",
    "\n",
    "- Please explain your methodology (modeling algorithm/method used and why, what features/data you found useful, what questions you have, and what you would do next with more time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef39f27",
   "metadata": {},
   "source": [
    "# Download the transaction file from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60322d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db903972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_json(\"https://github.com/CapitalOneRecruiting/DS/raw/master/transactions.zip\", lines=True )\n",
    "# df.to_csv(\"transaction.csv\", index=False)\n",
    "df = pd.read_csv(\"transaction.csv\",infer_datetime_format=True, encoding_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fa769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998a4cfc-760d-4a55-bc23-5c2b977346fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.transactionType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c152120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53fa78-5bac-4897-95e3-f4b6563bd55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of Row:\", df.shape[0])\n",
    "print(\"The number of columns:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3cf0b7-7681-447c-b87f-64abd30405e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e4160",
   "metadata": {},
   "source": [
    "- There is an indication of invalid entries in columns such as `echoBuffer`,`merchantCity`,`merchantState`,`merchantZip`,`posOnPremises`,`recurringAuthInd`\n",
    "- The columns listed above contain empty string values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e216df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transactionAmount'].plot(kind='hist',figsize = (12,8),title=\"Histogram of the Processed Amounts of each Transaction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39de9e",
   "metadata": {},
   "source": [
    "- It is observe that the distribution of the processed amount of each transaction is not normally distributed, all the transactions are center around 0 to 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba07eb",
   "metadata": {},
   "source": [
    "# Data Wragling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d15dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty strings with NaN\n",
    "df.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f60d4-712a-4318-a617-d05918c098ab",
   "metadata": {},
   "source": [
    "- **The original data type of transactionDateTime is an object, it needs to be converted to datetime.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"transactionDateTime\"]=pd.to_datetime(df[\"transactionDateTime\"]) # convert to appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbae025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12a1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get records of reversed transactions\n",
    "reversed_df = df[df['transactionType']=='REVERSAL']\n",
    "print(f'The total number of reversed transactions is {reversed_df.shape[0]}')\n",
    "print(f'while the total dollar amount estimated for the reversed transactions is ${reversed_df[\"transactionAmount\"].sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_of_interest = ['accountNumber',\"customerId\", 'transactionDateTime', 'cardLast4Digits', 'cardCVV',\n",
    "#                    'transactionAmount', 'transactionType', \"availableMoney\",\"merchantName\",\n",
    "#                    \"posEntryMode\",\"posConditionCode\",\"merchantCategoryCode\",\n",
    "#                    \"currentBalance\"]\n",
    "sorted_df = df.sort_values(by=['accountNumber',\"customerId\", 'cardLast4Digits','transactionDateTime', \"transactionAmount\"])\n",
    "\n",
    "# define a function to identify multiple swipes\n",
    "\n",
    "def identify_multiple_swipes(sub_df, time_window='2min'):\n",
    "    sub_df['transactionDateTime'] = pd.to_datetime(sub_df['transactionDateTime'])\n",
    "    sub_df['transactionDateTimeDiff'] = (sub_df['transactionDateTime'].shift(-1) - sub_df['transactionDateTime'])\n",
    "\n",
    "    sub_df['isMultipleSwipe'] = (\n",
    "        (sub_df['accountNumber'].eq(sub_df['accountNumber'].shift(-1))) &\n",
    "        (sub_df['customerId'].eq(sub_df['customerId'].shift(-1))) &\n",
    "        (sub_df['cardLast4Digits'].eq(sub_df['cardLast4Digits'].shift(-1))) &\n",
    "        (sub_df['transactionAmount'].eq(sub_df['transactionAmount'].shift(-1))) &\n",
    "        (sub_df['transactionType'].shift(-1)!= \"REVERSAL\") &\n",
    "        ((sub_df['transactionDateTime'].shift(-1) - sub_df['transactionDateTime']) <= pd.to_timedelta(time_window))\n",
    "    )\n",
    "   \n",
    "    return sub_df\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "sorted_df_new = identify_multiple_swipes(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac286f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4656826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to get only those transactions flagged as multiple swipes\n",
    "multiple_swipes_df = sorted_df_new[sorted_df_new['isMultipleSwipe']]\n",
    "# show\n",
    "multiple_swipes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5d30c-2ebd-4606-a32c-74a4b113a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually confirm\n",
    "sorted_df_new[(sorted_df_new['accountNumber']==100737756) &\n",
    "                (sorted_df_new['customerId']==100737756) & \n",
    "                (sorted_df_new['cardLast4Digits']==4317) & \n",
    "                (sorted_df_new['transactionAmount']==693.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294d5f3-b84a-4fdc-8d99-54ed2e90b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The total number of multiple swipe transactions is {multiple_swipes_df.shape[0]}')\n",
    "print(f'while the total dollar amount estimated for the multiple swipe transactions is ${multiple_swipes_df[\"transactionAmount\"].sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f3609",
   "metadata": {},
   "source": [
    "# Building The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c765d",
   "metadata": {},
   "source": [
    "### Data Cleaning and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataframe for cleaning\n",
    "data = sorted_df_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930eade4-d081-4ce1-a7e1-34ce27949a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicated records(reversed and multiple swipe transactions)\n",
    "data['transactionType'] = data['transactionType'].replace(\"REVERSAL\", np.nan)\n",
    "data['isMultipleSwipe'] = data['isMultipleSwipe'].replace(True, np.nan)\n",
    "data.dropna(subset=['transactionType', 'isMultipleSwipe'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba32468-449d-4924-b247-24d96b0cbfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['isMultipleSwipe'].unique() # check that isMultipleSwipe is only False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991e6e5-e4a7-4b4a-a6c9-b9f2efdc1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm there are no missing values in transactionType\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e79c6",
   "metadata": {},
   "source": [
    "**Looking at the missing value report above, it could be seen there are columns with 100% missing values: It will be ideal to remove such columns totally from the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fa5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage missing value\n",
    "pct_missing_before = ((data.isnull().sum()/data.isnull().count() * 100).sort_values(ascending=False)).round(2)\n",
    "print(\"BEFORE DROPPING COLUMNS WITH 100% MISSING VALUES: \\n\",pct_missing_before)\n",
    "\n",
    "# remove the columns with 100% missing values\n",
    "data.drop(columns=['echoBuffer', 'merchantCity', 'merchantState', 'merchantZip', 'posOnPremises', 'recurringAuthInd'], \n",
    "         inplace=True)\n",
    "\n",
    "# calculate percentage of missing value after droping the columns with 100% missing data \n",
    "pct_missing_after = ((data.isnull().sum()/data.isnull().count() * 100).sort_values(ascending=False)).round(2)\n",
    "print(\"\\nBEFORE DROPPING COLUMNS WITH 100% MISSING VALUES: \\n\",pct_missing_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6ff79-c758-40f6-a3fb-aa4f2394b6f5",
   "metadata": {},
   "source": [
    "Now that we have dropped the duplicated records and the columns with 100% missing values from the dataset, let's take a quick exploration on the structure of the cleaned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d9b873-1b8d-4dee-b7ac-a673cf544742",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d17bad-c1da-4994-8d8c-54f1e1b142d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the distribution of the target class\n",
    "data['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9544ab-53be-4090-ad90-9ec700549117",
   "metadata": {},
   "source": [
    "This indicates an imbalanced data. Let's visualize this distribution to get this better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca781e-4e8e-422c-a295-3650473b2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "sns.countplot(x='isFraud', data=df);\n",
    "plt.title(\"Target Variable Distribution\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2400a5b-25e6-4724-a4a1-65c14fbaffbf",
   "metadata": {},
   "source": [
    "It could be confirmed from the plot above that we have an imbalanced data which we will have to consider and handle in our model.\n",
    "\n",
    "\n",
    "\n",
    "Looking at the independent variables, it will be ideal excluding certain columns that might not provide useful information or could potentially lead to data leakage. **Here are some columns I consider excluding:**\n",
    "\n",
    "- `accountNumber`, `customerId`: These are identifiers and may not contribute to fraud prediction.\n",
    "\n",
    "- `cardCVV`, `enteredCVV`, `cardLast4Digits`: Features related to the card itself might not be very informative directly but we can derive a new feature that tells whether the cardCVV matches with the enteredCVV during purchase - `matchingCVV`.\n",
    "\n",
    "- `availableMoney`, `currentBalance`: These columns might not be available during the time of the transaction or could directly correlate with the `transactionAmount`, potentially causing **data leakage**.\n",
    "\n",
    "- `transactionDateTime`: While the timing of transactions is important, it's generally not a good idea to include the exact timestamp directly. We'll derive features such as `transactionHour`, `transactionDayOfWeek`, `transactionMonth`, and `transactionDayOfMonth`.\n",
    "\n",
    "- `merchantName`: This column may not be directly related to fraud and might introduce noise - since the number of merchant name that can exist is infinite. However if time permits, we might consider engineering features from it, such as the number of transactions from a specific merchant.\n",
    "\n",
    "- `currentExpDate`, `accountOpenDate`, `dateOfLastAddressChange`, `expirationDateKeyInMatch`: These date-related columns might not directly contribute to fraud prediction. but we can derive features such as the age of the account (`ageOfAccount`) or time since the last address change (`timeSinceLastAddressChange`).\n",
    "\n",
    "\n",
    "\n",
    "We'll see from the remaining variables the ones that have a unique interaction with the target variable to further justify our bases for selecting them for modeling.\n",
    "\n",
    "However, before we check for interaction, let's derive some new variables from existing variables, that could be of help to our model. These variables will be `transactionDayOfWeek`, `transactionMonth`, `transactionDayOfMonth`, `transactionHour`, `ageOfAccount`, `timeSinceLastAddressChange` and also, `matchingCVV` which tells if the cardCVV matches with the enteredCVV during purchase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294197ba-2ebc-4bdd-9351-695f4fdf85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract time related features\n",
    "data['transactionDayOfWeek'] = data['transactionDateTime'].dt.day_name()\n",
    "data['transactionMonth'] = data['transactionDateTime'].dt.month_name()\n",
    "data['transactionDayOfMonth'] = data['transactionDateTime'].dt.day\n",
    "data['transactionHour'] = data['transactionDateTime'].dt.hour\n",
    "\n",
    "data[\"accountOpenDate\"]=pd.to_datetime(data[\"accountOpenDate\"]) # convert to appropriate data type\n",
    "data[\"dateOfLastAddressChange\"]=pd.to_datetime(data[\"dateOfLastAddressChange\"]) # convert to appropriate data type\n",
    "data['ageOfAccount'] = (data['transactionDateTime'] - data['accountOpenDate']).dt.days\n",
    "data['timeSinceLastAddressChange'] = (data['transactionDateTime'] - data['dateOfLastAddressChange']).dt.days\n",
    "\n",
    "# get matchingCVV\n",
    "data['matchingCVV'] = (data['cardCVV'] ==data['enteredCVV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns not of interest\n",
    "cols_not_of_interest = ['accountNumber', 'customerId', 'transactionDateTime', 'cardCVV', 'enteredCVV', 'cardLast4Digits',\n",
    "                        'accountOpenDate', 'dateOfLastAddressChange', 'transactionDateTimeDiff', 'isMultipleSwipe',\n",
    "                       'merchantName', 'currentExpDate', 'availableMoney', 'currentBalance'\n",
    "                        ]\n",
    "data.drop(columns=cols_not_of_interest, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the missing values proper\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85c4ad",
   "metadata": {},
   "source": [
    "I will recommend uniquely identifying the missing values in these columns as they might not be missing at random. Categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13944e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['acqCountry'] = data['acqCountry'].replace(np.nan, 'MISSING')\n",
    "data['merchantCountryCode'] = data['merchantCountryCode'].replace(np.nan, 'MISSING')\n",
    "data['posEntryMode'] = data['posEntryMode'].replace(np.nan, '99')\n",
    "data['posConditionCode'] = data['posConditionCode'].replace(np.nan, '99')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28a6cf-d1b2-42a2-9a32-29dfa97db73d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify categorical and continuous independent variables, as well as the target variable:\n",
    "num_cols = ['creditLimit', 'transactionAmount', 'ageOfAccount','timeSinceLastAddressChange', 'transactionDayOfMonth']\n",
    "cat_cols = ['acqCountry', 'merchantCountryCode', 'posEntryMode', 'posConditionCode',\n",
    "            'merchantCategoryCode', 'transactionType', 'cardPresent',\n",
    "            'expirationDateKeyInMatch', 'transactionMonth', 'transactionDayOfWeek',\n",
    "            'transactionHour', 'matchingCVV']\n",
    "\n",
    "label_class = 'isFraud'\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,50))\n",
    "\n",
    "for i, col in enumerate(cat_cols):\n",
    "    ax1 = fig.add_subplot(6, 2, i+1)\n",
    "    x, y = col, \"proportion\"\n",
    "    (data[x]\n",
    "     .groupby(data[label_class])\n",
    "     .value_counts(normalize=True)\n",
    "     .rename(y)\n",
    "     .reset_index()\n",
    "     .pipe((sns.barplot, \"data\"), x=x, y=y, hue=label_class, ax=ax1))\n",
    "plt.title(\"Proportion plots for categorical independent variables against dependent variable \")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fde625",
   "metadata": {},
   "source": [
    "Some of the insights here are that:\n",
    "- We can tell transactions with card not present are more likely to be fraudulent\n",
    "- transaction with posEntryMode of 09, 02, 90 and 99 (missing) are more likely to be fraudulent while mode 05 is more likely legitimate transaction\n",
    "- Also missing posConditionMode (99) are more likely to be fraudulent\n",
    "- When there's non-matching CVV, the transaction has tendencies of being fraudulent\n",
    "\n",
    "\n",
    "Hence, the categorical variables that will be considered as a feature for the model are:\n",
    "- 'acqCountry'\n",
    "- 'merchantCountryCode'\n",
    "- 'posEntryMode'\n",
    "- 'posConditionCode',\n",
    "- 'merchantCategoryCode'\n",
    "- 'transactionType'\n",
    "- 'cardPresent'\n",
    "- 'transactionMonth'\n",
    "- 'transactionDayOfWeek',\n",
    "- 'transactionHour'\n",
    "- 'matchingCVV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26368a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,40))\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax1 = fig.add_subplot(5, 1, i+1)\n",
    "    sns.boxplot(x=label_class, y=col, data=data, ax=ax1)\n",
    "plt.title(\"Interaction between numeric/continuous independent variables against dependent variable \")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b59ad2",
   "metadata": {},
   "source": [
    "**Only transactionAmount shows a significant interaction with a transaction being fraudulent. Higher transaction amount are associated with fraudulent transactions. Hence only the transactionAmount will be considered from the continuous variables as a feature for the model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only columns of interest\n",
    "data = data[['acqCountry',\n",
    "            'merchantCountryCode',\n",
    "            'posEntryMode',\n",
    "            'posConditionCode',\n",
    "            'merchantCategoryCode',\n",
    "            'transactionType',\n",
    "            'cardPresent',\n",
    "            'transactionMonth',\n",
    "            'transactionDayOfWeek',\n",
    "            'transactionHour',\n",
    "            'matchingCVV','transactionAmount',\n",
    "            'isFraud']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a176ee0",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- preprocessing such as categorical encoding and ensuring all columns are of the numeric datatype\n",
    "- Handling imbalanced in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e70802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c9d68-a9e9-4e77-9bbf-27343e2ffbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() # check the data info again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e8c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy\n",
    "encoded_df = data.copy()\n",
    "# create a label encoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# fit and transform the data\n",
    "cols_to_encode = ['acqCountry', 'merchantCountryCode', 'merchantCategoryCode', \n",
    "                  'transactionType', 'transactionMonth', 'transactionDayOfWeek']\n",
    "\n",
    "for col in cols_to_encode:\n",
    "    encoded_df[col] = label_encoder.fit_transform(encoded_df[col])\n",
    "    # show the representation (check how the labelencoder has encoded the values)\n",
    "    print(dict(zip(label_encoder.inverse_transform(encoded_df[col].unique()), encoded_df[col].unique())))\n",
    "\n",
    "# make Boolean values in the data to 1's and 0's\n",
    "encoded_df.replace({False: 0, True: 1}, inplace=True)\n",
    "\n",
    "# convert all values to numeric\n",
    "encoded_df = encoded_df.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bed364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all columns are numeric\n",
    "encoded_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign independent and dependent variable\n",
    "X = encoded_df.drop(columns=[\"isFraud\"])\n",
    "y = encoded_df.isFraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfa18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data partitioning\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b315445",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape) # shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43920a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how the label classes have been distributed among the sets\n",
    "fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
    "# Count plot for training set\n",
    "sns.countplot(x=y_train, ax=axs[0])\n",
    "axs[0].set_title('Distribution of training data')\n",
    "axs[0].set_xlabel('Classes')\n",
    "# Count plot for test set\n",
    "sns.countplot(x=y_test, ax=axs[1])\n",
    "axs[1].set_title('Distribution of test data')\n",
    "axs[1].set_xlabel('Classes')\n",
    "\n",
    "print(\"training classes\\n\", pd.Series(y_train).value_counts())\n",
    "print(\"test classes\\n\", pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eba6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply random undersampling to the training set\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train classes\\n\", y_resampled.value_counts())\n",
    "sns.countplot(x=y_resampled)\n",
    "plt.title('Distribution of classes in the  data')\n",
    "plt.xlabel('Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89564ec-b512-45d2-a483-c7db834c22d3",
   "metadata": {},
   "source": [
    "**9. Scikit-Learn Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a8ce7-af78-420b-8a40-341ba2cba594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418436c-2a70-43fc-9c53-a096fc85d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr  = Pipeline([('scalar1',StandardScaler()),\n",
    "                         ('lr_classifier',LogisticRegression())])\n",
    "\n",
    "pipeline_knn = Pipeline([('scalar2',StandardScaler()),\n",
    "                          ('knn_classifier',KNeighborsClassifier())])\n",
    "\n",
    "pipeline_svc = Pipeline([('scalar3',StandardScaler()),\n",
    "                         ('svc_classifier',SVC())])\n",
    "\n",
    "pipeline_dt = Pipeline([('dt_classifier',DecisionTreeClassifier())])\n",
    "pipeline_rf = Pipeline([('rf_classifier',RandomForestClassifier(max_depth=3))])\n",
    "pipeline_gbc = Pipeline([('gbc_classifier',GradientBoostingClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef068787-d99e-483b-950d-1d94bd7e8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [pipeline_lr,\n",
    "            pipeline_knn,\n",
    "            pipeline_svc,\n",
    "            pipeline_dt,\n",
    "            pipeline_rf,\n",
    "            pipeline_gbc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed2703-ce11-47ea-8b8e-502768b16135",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175b10a-80b1-45d3-b611-d31ace59bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipe in pipelines:\n",
    "    pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51058eee-63f7-416f-9159-91d9fd5b5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dict = {0:'LR',\n",
    "             1:'KNN',\n",
    "             2:'SVC',\n",
    "             3:'DT',\n",
    "             4: 'RF',\n",
    "             5: 'GBC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76e363-8355-4394-8795-569c0c188f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10560c9-7552-4925-96e5-af03cf6cbf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,model in enumerate(pipelines):\n",
    "    print(\"{} Test Accuracy:{}\".format(pipe_dict[i],model.score(X_test,y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b34de-722c-444d-96a2-d4bc87e9986e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# create a Random Forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# train the model on the resampled training data\n",
    "random_forest.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a37297-f1dd-4768-82c0-2365fdea65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_forest =RandomForestClassifier(max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c593bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn import metrics\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "f1_score = metrics.f1_score(y_test,y_pred)\n",
    "class_report = metrics.classification_report(y_test,y_pred)\n",
    "conf_mat = metrics.confusion_matrix(y_test,y_pred) \n",
    "\n",
    "print(\"F1 score: %.3f\"%(f1_score))\n",
    "print(\"Classification report: \\n\",class_report)\n",
    "\n",
    "fig, ax = plot_confusion_matrix(conf_mat=conf_mat,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True)\n",
    "plt.yticks([0, 1], ['False', 'True'])\n",
    "plt.xticks([0, 1], ['False', 'True'])   \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8603e2b6-3811-4712-a64d-e67b849f7810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance (plot graph of feature importances for better visualization)\n",
    "feat_importances = pd.Series(random_forest.feature_importances_, index=X_resampled.columns)\n",
    "(100*feat_importances).nsmallest(44).plot(kind='barh', figsize=(12,12))\n",
    "plt.grid()\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.xlabel(\"Importance (%)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d5b7e",
   "metadata": {},
   "source": [
    "If I have more time:\n",
    "- I will do more feature engineering and interaction to see they can help improve the model e.g getting the length of hours between the last address changed date and the transaction date.\n",
    "- include other features that may capture the context of the transaction, such as:\n",
    "    - Whether the transaction amount is close to the credit limit\n",
    "    - Whether the transaction amount is significantly different from the average transaction amount for that account\n",
    "- hyperparameter tuning/optimization and also compare more models.\n",
    "- I will optimize the code and create pipeline for easy deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe972e35-6858-4b94-91e6-f3e3b75d7598",
   "metadata": {},
   "source": [
    "**Prediction on New Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1bede6-c3ef-408a-a829-805c374bf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data2 = pd.DataFrame(new_data.values, columns=X_resampled.columns)\n",
    "\n",
    "# new_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a042d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame({\n",
    "    'acqCountry':33.6,\n",
    "     'merchantCountryCode':50,\n",
    "    'posEntryMode':33.6,\n",
    "    'posConditionCode':0.627,\n",
    "    'merchantCategoryCode':35.0,\n",
    "    'transactionType':50,\n",
    "    'cardPresent':50,\n",
    "    'transactionMonth':72.0,\n",
    "    'transactionDayOfWeek':79.799479,\n",
    "    'transactionHour':148.0,\n",
    "    'matchingCVV':0.627,\n",
    "    'transactionAmount':6\n",
    "  \n",
    "},index=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f012c-764f-4d81-a8be-bafc35de418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff44db-4a16-42b1-b6bd-f79b00d38dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = random_forest.predict(new_data)\n",
    "#p = rf.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29346b8-dffd-4e3d-9309-3a9df874f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1df13-f6b9-47ad-870c-1d59c1aec161",
   "metadata": {},
   "outputs": [],
   "source": [
    "if p[0] == 0:\n",
    "    print('Not Fraud')\n",
    "else:\n",
    "    print('Fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89f590-cf77-4de8-8620-656e5683716b",
   "metadata": {},
   "source": [
    "**18. Save Model Using Joblib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e35517-00d4-481d-b99d-6adf89cfd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f7631-b8cc-4129-9355-a171a1cebb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(random_forest,'model_Capital_Bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb82517-4666-4610-af7f-0c2ad0473c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('model_Capital_Bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74062b-10bf-4fd9-a802-eb27d13c8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d09db-76f1-4ecf-933e-52a39137f8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca0fb97d-9358-4f39-897a-bf624a77564e",
   "metadata": {},
   "source": [
    "**GUI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebba30-85ed-4aa0-b744-73177af6a925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c887a5-f3c2-4cfe-8fb2-745f18841d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8902eaa-4692-4877-b8c5-789434c16be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "def show_entry_fields():\n",
    "    p1=float(e1.get())\n",
    "    p2=float(e2.get())\n",
    "    p3=float(e3.get())\n",
    "    p4=float(e4.get())\n",
    "    p5=float(e5.get())\n",
    "    p6=float(e6.get())\n",
    "    p7=float(e7.get())\n",
    "    p8=float(e8.get())\n",
    "    p9=float(e9.get())\n",
    "    p10=float(e10.get())\n",
    "    p11=float(e11.get())\n",
    "    p12=float(e12.get())\n",
    "\n",
    "    \n",
    "   \n",
    "    model = joblib.load('model_Capital_Bank')\n",
    "    result=model.predict([[p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12]])\n",
    "    \n",
    "    if result == 0:\n",
    "        Label(master, text=\"Not Fraud\").grid(row=31)\n",
    "    else:\n",
    "        Label(master, text=\"Fraud\").grid(row=31)\n",
    "    \n",
    "    \n",
    "master = Tk()\n",
    "master.title(\"Capital One Bank Fraud Detection Using Machine Learning\")\n",
    "\n",
    "\n",
    "label = Label(master, text = \"Capital One Bank Fraud Detection Using Machine Learning\"\n",
    "                          , bg = \"black\", fg = \"white\"). \\\n",
    "                               grid(row=0,columnspan=2)\n",
    "\n",
    "\n",
    "Label(master, text=\"Enter Value of acqCountry\").grid(row=1)\n",
    "Label(master, text=\"Enter Value of merchantCountryCode\").grid(row=2)\n",
    "Label(master, text=\"Enter Value of posEntryMode\").grid(row=3)\n",
    "Label(master, text=\"Enter Value of posConditionCode\").grid(row=4)\n",
    "Label(master, text=\"Enter Value of merchantCategoryCode\").grid(row=5)\n",
    "Label(master, text=\"Enter Value of transactionType\").grid(row=6)\n",
    "Label(master, text=\"Enter Value of cardPresent\").grid(row=7)\n",
    "Label(master, text=\"Enter Value of transactionMonth\").grid(row=8)\n",
    "Label(master, text=\"Enter Value of transactionDayOfWeek\").grid(row=9)\n",
    "Label(master, text=\"Enter Value of transactionHour\").grid(row=10)\n",
    "Label(master, text=\"Enter Value of matchingCVV\").grid(row=11)\n",
    "Label(master, text=\"Enter Value of transactionAmount\").grid(row=12)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "e1 = Entry(master)\n",
    "e2 = Entry(master)\n",
    "e3 = Entry(master)\n",
    "e4 = Entry(master)\n",
    "e5 = Entry(master)\n",
    "e6 = Entry(master)\n",
    "e7 = Entry(master)\n",
    "e8 = Entry(master)\n",
    "e9 = Entry(master)\n",
    "e10 = Entry(master)\n",
    "e11 = Entry(master)\n",
    "e12 = Entry(master)\n",
    "\n",
    "e1.grid(row=1, column=1)\n",
    "e2.grid(row=2, column=1)\n",
    "e3.grid(row=3, column=1)\n",
    "e4.grid(row=4, column=1)\n",
    "e5.grid(row=5, column=1)\n",
    "e6.grid(row=6, column=1)\n",
    "e7.grid(row=7, column=1)\n",
    "e8.grid(row=8, column=1)\n",
    "e9.grid(row=9, column=1)\n",
    "e10.grid(row=10, column=1)\n",
    "e11.grid(row=11, column=1)\n",
    "e12.grid(row=12, column=1)\n",
    "\n",
    "\n",
    "Button(master, text='Predict', command=show_entry_fields).grid()\n",
    "\n",
    "mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05613db7-77bd-4e2e-a2f0-ff0f8b38ac7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accfa3ff-5c91-41b9-8ff3-a41a60bdee34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
